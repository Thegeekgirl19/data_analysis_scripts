{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "from sklearn.metrics import classification_report, confusion_matrix"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Initialize an empty list to store data for different thresholds"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["thresholds = [0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95]\n", "data_list = []"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for threshold in thresholds:\n", "    column_name = f'result_thr_{int(threshold * 100)}'\n\n", "    # Apply the threshold to create a new column in the original DataFrame\n", "    aug_pred_prob_byrd_70_epic[column_name] = aug_pred_prob_byrd_70_epic.apply(lambda row: row['predicted_class'] if row['max_prob'] > threshold else 0, axis=1)\n", "    \n", "    # Filter the DataFrame based on the new column\n", "    test_threshold = aug_pred_prob_byrd_70_epic[aug_pred_prob_byrd_70_epic[column_name] != 0]\n", "    \n", "    # Calculate classification report and confusion matrix\n", "    report = classification_report(test_threshold['EANDM_Final_code_1'], test_threshold[column_name], output_dict=True)\n", "    conf_matrix = confusion_matrix(test_threshold['EANDM_Final_code_1'], test_threshold[column_name])\n", "    \n", "    # Create a data dictionary for this threshold\n", "    data = {\n", "        'Threshold': [threshold],\n", "        'Accuracy': [report['accuracy']],\n", "        'Coverage': [len(test_threshold) / len(aug_pred_prob_byrd_70_epic)],\n", "        'Total': [len(aug_pred_prob_byrd_70_epic)]\n", "    }\n\n", "        # Add 'macro avg' and 'weighted avg' to the data dictionary\n", "    data['Support'] = [report['macro avg']['support']]\n", "    data['macro avg_F1_Score'] = [report['macro avg']['f1-score']]\n", "    data['weighted avg_F1_Score'] = [report['weighted avg']['f1-score']]\n", "    \n", "    # Extract precision, recall, f1-score, and support for each class\n", "    for class_name in report.keys():\n", "        if class_name not in ['accuracy', 'weighted avg', 'macro avg']:\n", "            data[class_name + '_Precision'] = [report[class_name]['precision']]\n", "            data[class_name + '_Recall'] = [report[class_name]['recall']]\n", "            data[class_name + '_F1_Score'] = [report[class_name]['f1-score']]\n", "            data[class_name + '_Support'] = [report[class_name]['support']]\n", "    \n", "    # Add 'macro avg' and 'weighted avg' to the data dictionary\n", "    # data['macro avg_F1_Score'] = [report['macro avg']['f1-score']]\n", "    # data['weighted avg_F1_Score'] = [report['weighted avg']['f1-score']]\n", "    # data['Support'] = [report['macro avg']['support']]\n", "    \n", "    \n", "    # Append the data for this threshold to the list\n", "    data_list.append(data)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create the final DataFrame by concatenating the data for different thresholds"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.concat([pd.DataFrame(data) for data in data_list], ignore_index=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Display the final DataFrame"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}