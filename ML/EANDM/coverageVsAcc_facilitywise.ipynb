{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "from sklearn.metrics import classification_report, confusion_matrix"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Your DataFrame<br>\n", "df = pd.read_csv('your_data.csv')  # Replace 'your_data.csv' with the actual CSV file path"]}, {"cell_type": "markdown", "metadata": {}, "source": ["List of facility codes"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["facility_codes_to_apply = [7198, 5645, 90, 29, 2586, 2492, 6958, 3228, 21, 3232, 2498, 1388, 45, 3666, 2497, 36, 5743, 120936, 2464]\n", "facility_codes_to_apply_70 = [213, 5719, 1163, 2541, 10267, 5668, 8697, 282, 5617, 1198, 475744, 10941, 70, 205062, 183, 494582, 5699]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Function to apply to each row"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def apply_new_column(row):\n", "    if row['facility_code'] in facility_codes_to_apply:\n", "        return 2492\n", "    elif row['facility_code'] in facility_codes_to_apply_70:\n", "        return 70  # Or any other value you prefer for non-matching facility codes\n", "    else:\n", "        return 488965"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Apply the function to create the new column"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["aug_pred_prob_byrd_70_epic['main_facility'] = aug_pred_prob_byrd_70_epic.apply(apply_new_column, axis=1)\n", "# Your DataFrame\n", "# aug_pred_prob_byrd_70_epic = pd.read_csv('your_data.csv')  # Replace 'your_data.csv' with the actual CSV file path"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Initialize an empty list to store data for different thresholds"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["thresholds = [0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90, 0.95]\n", "data_list = []"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Iterate over unique facilities"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for facility in aug_pred_prob_byrd_70_epic['main_facility'].unique():\n", "    facility_df = aug_pred_prob_byrd_70_epic[aug_pred_prob_byrd_70_epic['main_facility'] == facility]\n", "    \n", "    for threshold in thresholds:\n", "        column_name = f'result_thr_{int(threshold * 100)}'\n", "    \n", "        # Apply the threshold to create a new column in the filtered DataFrame\n", "        facility_df[column_name] = facility_df.apply(lambda row: row['predicted_class'] if row['max_prob'] > threshold else 0, axis=1)\n", "        \n", "        # Filter the DataFrame based on the new column\n", "        test_threshold = facility_df[facility_df[column_name] != 0]\n", "        \n", "        # Calculate classification report and confusion matrix\n", "        report = classification_report(test_threshold['EANDM_Final_code_1'], test_threshold[column_name], output_dict=True)\n", "        \n", "        # Create a data dictionary for this threshold\n", "        data = {\n", "            'facility': [facility],\n", "            'Threshold': [threshold],\n", "            'Accuracy': [report['accuracy']],\n", "            'Coverage': [len(test_threshold) / len(facility_df)],\n", "            'Total': [len(facility_df)]\n", "        }\n", "                # Add 'macro avg' and 'weighted avg' to the data dictionary\n", "        data['Support'] = [report['macro avg']['support']]\n", "        data['macro avg_F1_Score'] = [report['macro avg']['f1-score']]\n", "        data['weighted avg_F1_Score'] = [report['weighted avg']['f1-score']]\n", "    \n", "        # Extract precision, recall, f1-score, and support for each class\n", "        for class_name in report.keys():\n", "            if class_name not in ['accuracy', 'weighted avg', 'macro avg']:\n", "                data[class_name + '_Precision'] = [report[class_name]['precision']]\n", "                data[class_name + '_Recall'] = [report[class_name]['recall']]\n", "                data[class_name + '_F1_Score'] = [report[class_name]['f1-score']]\n", "                data[class_name + '_Support'] = [report[class_name]['support']]\n", "        \n", "        \n", "        # Append the data for this threshold to the list\n", "        data_list.append(data)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create the final DataFrame by concatenating the data for different thresholds"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.concat([pd.DataFrame(data) for data in data_list], ignore_index=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Display the final DataFrame"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}